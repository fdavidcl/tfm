% Plantilla para un Trabajo Fin de Grado de la Universidad de Granada,
% adaptada para el Doble Grado en Ingeniería Informática y Matemáticas.
%
%  Autor: Mario Román.
%  Licencia: GNU GPLv2.
%
% Esta plantilla es una adaptación al castellano de la plantilla
% classicthesis de André Miede, que puede obtenerse en:
%  https://ctan.org/tex-archive/macros/latex/contrib/classicthesis?lang=en
% La plantilla original se licencia en GNU GPLv2.
%
% Esta plantilla usa símbolos de la Universidad de Granada sujetos a la normativa
% de identidad visual corporativa, que puede encontrarse en:
% http://secretariageneral.ugr.es/pages/ivc/normativa
%
% La compilación se realiza con las siguientes instrucciones:
%   pdflatex --shell-escape main.tex
%   bibtex main
%   pdflatex --shell-escape main.tex
%   pdflatex --shell-escape main.tex

% Opciones del tipo de documento
\documentclass[oneside,openright,titlepage,numbers=noenddot,openany,headinclude,footinclude=true,
cleardoublepage=empty,abstractoff,BCOR=5mm,paper=a4,fontsize=12pt,main=spanish]{scrreprt}

% Paquetes de latex que se cargan al inicio. Cubren la entrada de
% texto, gráficos, código fuente y símbolos.
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{subcaption}
\expandafter\def\csname ver@subfig.sty\endcsname{}

\usepackage[final]{pdfpages}
\usepackage{fixltx2e}
\usepackage{graphicx} % Inclusión de imágenes.
\usepackage{grffile}  % Distintos formatos para imágenes.
\usepackage{longtable} % Tablas multipágina.
\usepackage{wrapfig} % Coloca texto alrededor de una figura.
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage[colorlinks=true]{hyperref}
\usepackage{tikz} % Diagramas conmutativos.
\usepackage{minted} % Código fuente.
\usepackage[T1]{fontenc}
%\usepackage{natbib}

\usepackage{lscape,caption} 
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools, cuted}
%\usepackage{graphicx}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algpseudocode}
%\usepackage{color}
\usepackage{booktabs}
\usepackage{stmaryrd}
\usepackage{epstopdf}
\usepackage{pdflscape}
\usepackage{stmaryrd}
%\usepackage[table]{xcolor}
\usepackage{array}
%\usepackage{tikz}
\usepackage{caption}

\usepackage{dblfloatfix}
\usepackage{flushend}

\setcounter{tocdepth}{1}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\graphicspath{{./}{inffus/}}

% Plantilla classicthesis
\usepackage[beramono,eulerchapternumbers,linedheaders,parts,a5paper,dottedtoc,
manychapters,pdfspacing]{classicthesis}

% Geometría y espaciado de párrafos.
\setcounter{secnumdepth}{0}
\usepackage{enumitem}
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\setlist[enumerate]{topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex}
\usepackage[top=1in, bottom=1.5in, left=1in, right=1in]{geometry}
\setlength\itemsep{0em}
\setlength{\parindent}{0pt}
\usepackage{parskip}

% Profundidad de la tabla de contenidos.
\setcounter{secnumdepth}{3}

% Usa el paquete minted para mostrar trozos de código.
% Pueden seleccionarse el lenguaje apropiado y el estilo del código.
\usepackage{minted}
\usemintedstyle{colorful}
\setminted{fontsize=\small}
\setminted[haskell]{linenos=false,fontsize=\small}
\renewcommand{\theFancyVerbLine}{\sffamily\textcolor[rgb]{0.5,0.5,1.0}{\oldstylenums{\arabic{FancyVerbLine}}}}

% Archivos de configuración.
\input{macros}  % En macros.tex se almacenan las opciones y comandos para escribir matemáticas.
\input{classicthesis-config} % En classicthesis-config.tex se almacenan las opciones propias de la plantilla.

% Color institucional UGR
% \definecolor{ugrColor}{HTML}{ed1c3e} % Versión clara.
\definecolor{ugrColor}{HTML}{c6474b}  % Usado en el título.
\definecolor{ugrColor2}{HTML}{c6474b} % Usado en las secciones.

% Datos de portada
\usepackage{titling} % Facilita los datos de la portada
\author{Francisco David Charte Luque} 
\date{\today}
\title{Generación de representaciones alternativas\\de datos mediante técnicas no supervisadas\\de Deep Learning\\\vspace{1em}Generating alternative data representations\\with unsupervised deep learning techniques}

% Portada
\include{titlepage}
\usepackage{wallpaper}
%\usepackage[main=english]{babel}

\DeclareRobustCommand{\chaptertitle}[1]{\textls[80]{\scshape #1}}


\begin{document}

\ThisULCornerWallPaper{1}{ugrA4.pdf}
\maketitle
\tableofcontents

\clearpage

\chapter*{Abstract}
\addcontentsline{toc}{chapter}{\chaptertitle{abstract}}

The present Master's final project tackles the problem of generating alternative representations from a data set, by using unsupervised feature extraction methods based on deep learning, both theoretically and from a more practical approach. For this purpose, firstly it presents an article studying neural structures which allow to learn features, called \textit{autoencoders}.  After this, a software library developed in the context of the studied field is described.

\autoref{p.theory} of the present work consists in an article published in the Information Fusion journal (Q1 in JCR). It defined the diverse concepts related to the mechanisms to build features out of the data:
\begin{itemize}
\item Feature engineering
\item Feature learning
\item Representation learning
\item Feature selection
\item Feature extraction
\item Feature fusion
\end{itemize}

In addition to this, it presents a specific class of neural networks designed for these tasks, autoencoders. The components of a basic autoencoder are described: its neural structure, activation functions and other architecture traits. This description follows with a taxonomy of the different autoencoder models available in the literature, according to the properties they induce in the learned encodings. An analysis follows with the foundations and properties of the main variants: basic, sparse, contractive, denoising and robust, as well as other kinds of autoencoder with specific applications, such as generative models.

To provide the reader with a context, autoencoders are compared to other classical feature learning techniques, linear as well as non-linear, and application fields are introduced. Lastly, some guidelines are included for the design of autoencoders for specific tasks, enumerating the more relevant software libraries which can be used as basis for the developments, and a case study compares different options and variants.


In \autoref{p.practice}, as a practical development in the studied topic a software package implementing the main autoencoder models has been developed. The tool, named Ruta,  is published on the CRAN repository for R. It consists in a complete library with 6 autoencoder variants, based in the well-known deep learning framework Keras.

Its functionality includes the definition of neural structures, the selection of an autoencoder type and its objective function, the training process with several customizable parameters and its evaluation against a test set. This document describes usage and functionality for each available method in the developed software.

To conclude, \autoref{p.final} summarizes the obtained results and outlines some future work for a doctoral thesis. 

\paragraph{Keywords} Machine learning, Deep Learning, unsupervised learning, neural networks, feature fusion.

\chapter*{Resumen}
\addcontentsline{toc}{chapter}{\chaptertitle{resumen}}

El presente trabajo fin de máster aborda el problema de generación de representaciones a partir de un conjunto de datos, mediante el uso de métodos de transformación y extracción de características no supervisados basados en aprendizaje profundo. Para ello, primero se presenta un artículo en el que se estudian las estructuras neuronales que permiten aprender características, denominadas \textit{autoencoders}. Después, se describe un software desarrollado en el contexto del ámbito estudiado que implementa dichas estructuras.

La Parte \ref{p.theory} del presente trabajo consiste en un artículo publicado en la revista Information Fusion (primer cuartil en JCR). Este define los distintos conceptos relacionados con los mecanismos para construir características a partir de los datos:
\begin{itemize}
\item Ingeniería de características
\item Aprendizaje de características
\item Aprendizaje de representaciones
\item Selección de características
\item Extracción de características
\item Fusión de características
\end{itemize}
Además, se introduce una clase específica de redes neuronales diseñadas para estas tareas, los autoencoders. Se detallan las componentes de un autoencoder básico: la estructura neuronal, las funciones de activación, y las características de la arquitectura. La descripción sigue con una taxonomía de los diferentes modelos de autoencoder disponibles, de acuerdo a las propiedades que inducen en las codificaciones aprendidas. A continuación, se analizan los fundamentos y propiedades de las variantes principales: básico, \textit{sparse}, \textit{contractive}, \textit{denoising} y \textit{robust}, y se mencionan otros tipos de autoencoder con aplicaciones específicas, como los modelos generativos.

Para proporcionar un contexto, se comparan los autoencoders con otras técnicas clásicas de aprendizaje de características, tanto lineales como no lineales, y se presentan los diversos campos de aplicación en los que se usan. Por último, se incluyen unas indicaciones para la hora de diseñar un autoencoder para una tarea concreta, mencionando las distintas librerías software que se pueden utilizar como base para los desarrollos, y se realiza un caso de uso en el que se comparan las distintas opciones y variantes.

En la Parte \ref{p.practice}, como desarrollo práctico del tema estudiado se ha elaborado un paquete software que implementa los modelos principales de autoencoder, denominado Ruta y publicado en el repositorio CRAN de librerías para el lenguaje R. Se trata de una completa librería con 6 variantes del autoencoder, basada en el conocido framework para aprendizaje profundo Keras.

La funcionalidad del paquete incluye la definición de las estructuras neuronales, la selección del tipo de autoencoder y de la función a optimizar, el entrenamiento con varios parámetros personalizables y su evaluación contra un conjunto de test. En este trabajo se describe el uso y funcionalidad de cada método disponible en el software desarrollado.

Para concluir, este trabajo recoge en la Parte \ref{p.final} los resultados obtenidos y presenta las vías de investigación que se plantean para una futura tesis doctoral.

\paragraph{Palabras clave} Aprendizaje automático, Deep Learning, aprendizaje no supervisado, redes neuronales, fusión de características.

\ctparttext{
  \color{black}
  \begin{center}
    A theoretical analysis of unsupervised deep learning models which generate alternative representations of data. 
  \end{center}
}
\part{Theory}
\label{p.theory}

%\chapter{Introduction}

%The increase in information generation and the need for its processing during the latest years has caused the development of diverse techniques for automatic non-trivial knowledge extraction from those data. The discipline which studies and develops these methods is known as \textit{data mining}.

%A fundamental issue in data processing is its quality: the presence of noise, the amount of measured features or class overlapping are traits which can determine the performance of a learning model. In order to ensure that the data we are working on have sufficient quality, it is usual to carry out a previous \textit{preprocessing} stage during which data are analyzed and transformed: normalizations, instance and feature reduction, noise filtering, etc. are some of the alterations which can be applied to data in this phase.



\chapter{A practical tutorial on autoencoders for nonlinear feature fusion: Taxonomy, models, software and guidelines}


\input{inffus/AutoencoderReview.tex}

%% \chapter{Sección segunda}

%% \includepdf[pages=-]{publicacion.pdf}

\ctparttext{\color{black}\begin{center}
Description and documentation of the developed software implementing the previously analyzed techniques.
\end{center}}

\part{Practical development}
\label{p.practice}

\chapter{Description of the developed software}

Ruta is a software library for designing and training unsupervised neural architectures, namely autoencoders. It is based in the well known open source deep learning library Keras \cite{Keras} and its R interface. It has been developed to work with the TensorFlow \cite{Tensorflow} backend.

\section{Installation}

In order to install the dependencies the Python interpreter will be needed as well; Tensorflow and Keras can be installed either via the Python package manager \texttt{pip} or possibly the distro’s package manager in the case of Linux.
\begin{minted}[frame=lines]{sh}
$ sudo pip install tensorflow
$ sudo pip install keras
\end{minted}

Then,  Ruta can be easily obtained from the CRAN by writing the following command into an R console:
\begin{minted}[frame=lines]{R}
install.packages("ruta")  
\end{minted}

Alternatively, the latest development version can be downloaded from GitHub:
\begin{minted}[frame=lines]{R}
devtools::install_github("fdavidcl/ruta")
\end{minted}

All R dependencies will be automatically installed. These include the Keras R interface and \texttt{purrr}. For convenience we also recommend installing and loading either \texttt{magrittr} or \texttt{purrr}, so that the pipe operator \texttt{\%>\%} is available.

\section{Usage}

The easiest way to start working with Ruta is to use the \texttt{autoencode()} function. It allows for selecting a type of autoencoder and transforming the feature space of a data set onto another one with some desirable properties depending on the chosen type.
\begin{minted}[frame=lines]{R}
iris[, 1:4] %>% as.matrix %>% autoencode(2, type = "denoising")  
\end{minted}

Ruta provides the functionality to build diverse neural architectures (with function \texttt{autoencoder()}), train them as autoencoders (\texttt{train()}) and perform different tasks with the resulting models (\texttt{reconstruct()}), including evaluation (\texttt{evaluate\_X()}). The following is a basic example of a natural pipeline with an autoencoder:
\begin{minted}[frame=lines]{R}
library(ruta)
library(purrr)

# Shuffle and normalize dataset
x <- iris[, 1:4] %>% sample %>% as.matrix %>% scale
x_train <- x[1:100, ]
x_test <- x[101:150, ]

autoencoder(
  input() + dense(256) + dense(36, "tanh") + dense(256) + output("sigmoid"),
  loss = "mean_squared_error"
) %>%
  make_contractive(weight = 1e-4) %>%
  train(x_train, epochs = 40) %>%
  evaluate_mean_squared_error(x_test)
\end{minted}

For more details, other examples and the full documentation can be found on \autoref{c.ref} as well as the project web site, \url{https://ruta.software}.

\chapter{User manual of Ruta 1.0.2}
\label{c.ref}

\input{reference/reference.tex}

\part{Results and conclusions}
\label{p.final}

\chapter{Future work}

As can be seen from the theoretical work in \autoref{p.theory}, autoencoders are a tool with many applications as data preprocessing tools. Continuing with this approach, we will study the possibilities of autoencoders and other similar neural structures applied to non-standard learning problems, which can be defined as problems whose data representation does not fit the traditional pattern: instances described each by one vector with values in several input attributes and at most one output attribute. The specific non-standard learning problems tackled by our future work will be: multi-instance, multi-view, multi-label, multi-dimensional, multi-target regression and label distribution learning.

An additional course of action when designing new methods for these problems will be the construction of ensembles, made up by neural architectures which will then produce different trained models. These will potentially combine into a more powerful model for the data. We will take into account the need for diverse models and the multiple options for their integration.

The practical work done with package Ruta (\autoref{p.practice}) will be thoroughly extended, to provide not only training and evaluation of customizable autoencoders, but also generation of meaningful visualizations which illustrate the training process as well as learned weights and resulting encodings. We plan on grouping all this functionality in a new package called Rutavis, aimed at understanding the behavior of these methods. Further usefulness could be brought to this software suite with a web-based graphical user interface providing easy access to both models and visualizations.


\chapter{Ending comments}

The contributions from the work during the completion of this project have been twofold: a paper and a software package.

First, a review paper gathering the main autoencoder models in a taxonomy has been published on a scientific journal of notable impact: Information Fusion, ranked Q1 in JCR. This paper describes autoencoder variants, compares them to traditional feature learning techniques and provides guidelines on how to use them.

Secondly, a free open source piece of software, Ruta, has been developed, enabling users to quickly define, train and evaluate very diverse autoencoder models. It has been published in the main repository for the language R, the CRAN. This library paves the way for future advances in understanding and designing architectures which adapt to different problems.

The present document groups and contextualizes both of these accomplishments, serving as a starting point for future research in the various scopes described in the previous chapter.

% Añade sección de referencias al final del documento.
% Selecciona un estilo de cita.
\bibliographystyle{elsarticle-num}
% En research.bib están las entradas de los artículos que citamos.
% Podemos cambiar el nombre del archivo aquí.
\bibliography{references}   

\end{document}
